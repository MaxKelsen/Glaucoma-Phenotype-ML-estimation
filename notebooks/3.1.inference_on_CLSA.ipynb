{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on CLSA dataset\n",
    "\n",
    "This file is part of the Glaucoma Phenotype ML Estimation project.\n",
    "\n",
    " Glaucoma Phenotype ML Estimation is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "\n",
    "The Glaucoma Phenotype ML Estimation project is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with the Glaucoma Phenotype ML Estimation project.  If not, see <http://www.gnu.org/licenses/>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Prelimanry information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this as required\n",
    "import os\n",
    "os.chdir('/Users/kaiahsteven/Desktop/Work/IOP/glaucoma/glaucoma_ML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import os\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "#import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from fastai.vision import *\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import zipfile\n",
    "from fastai.distributed import *\n",
    "from glaucoma.helpers.glaucoma_helpers import *\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PLEASE SET AS REQUIRED########\n",
    "WORKING_DIR = Path(os.getcwd())\n",
    "DATA_DIR = WORKING_DIR / 'data'\n",
    "META_DIR = DATA_DIR / 'metadata'\n",
    "TRAIN_DIR = DATA_DIR / 'train'\n",
    "UKBB_DIR = DATA_DIR 'retinal_images/UKBB'\n",
    "CROP_DIR = DATA_DIR /'retinal_images/cropped_UKBB')\n",
    "C_CLSA_DIR = DATA_DIR / 'retinal_images/cropped_CLSA')\n",
    "GRADE_DIR = DATA_DIR / \"gradable\"\n",
    "CLSA_DIR =DATA_DIR / 'retinal_images/CLSA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzipping files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_files = parse_files(CLSA_DIR)\n",
    "f_names = [os.path.basename(zf)[0:-4] for zf in zipped_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = CLSA_DIR / f_names[0]/ '190225'\n",
    "followup = CLSA_DIR / f_names[1]/'190225'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_right = parse_files(baseline, 'right')\n",
    "baseline_left = parse_files(baseline, 'left')\n",
    "followup_right = parse_files(followup, 'right')\n",
    "followup_left = parse_files(followup, 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(baseline_right[0])\n",
    "width, height = im.size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "#img=mpimg.imread(ukbb_images_right[0])\n",
    "imgplot = plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = (1536-350-800)/1536\n",
    "h2 = (1536-350)/1536\n",
    "w1 = (2048 - 1040 -200)/2048\n",
    "w2 = (2048 - 200)/2048\n",
    "hc_1 = height * h1\n",
    "hc_2 = height * h2\n",
    "wc_1 = width * w1\n",
    "wc_2 = width * w2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_ratio_w = 1080 * width/2048\n",
    "same_ratio_h = 800 *width/2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_1 =200 * width/2048\n",
    "wc_1 = 350 *width/2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_CLSA(f_name, crop =True, left =True ):\n",
    "    im = Image.open(f_name)\n",
    "    width, _ = im.size\n",
    "    if crop:\n",
    "        same_ratio_w = 1080 * width/2048\n",
    "        same_ratio_h = 800 *width/2048\n",
    "        hc_1 = 200 * width/2048\n",
    "        if left:\n",
    "            wc_1 = 350 *width/2048\n",
    "        else:\n",
    "            wc_1 = 800 *width/2048\n",
    "        width_offset = 0\n",
    "        left =  wc_1 +width_offset\n",
    "        top = hc_1\n",
    "        right = wc_1 +same_ratio_w +width_offset\n",
    "        bottom = hc_1 + same_ratio_h \n",
    "        im = im.crop((left,top,right,bottom))\n",
    "    plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(baseline_left[10])\n",
    "width_offset = 0\n",
    "left =  wc_1 +width_offset\n",
    "top = hc_1\n",
    "right = wc_1 +same_ratio_w +width_offset\n",
    "bottom = hc_1 + same_ratio_h \n",
    "iml = im.crop((left,top,right,bottom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(iml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cropping files\n",
    "width_offset = 0\n",
    "left = wc_1 + width_offset\n",
    "top = hc_1\n",
    "right = wc_1 +same_ratio_w +width_offset\n",
    "bottom = hc_1 + same_ratio_h \n",
    "save_path = C_CLSA_DIR / 'baseline_left'\n",
    "crop_files(baseline_left,(left,top,right,bottom),save_path,num_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running gradabillity model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_img = GRADE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the gradabillity models\n",
    "src = ImageList.from_folder(str(path_img)).split_by_rand_pct(seed=42).label_from_folder()\n",
    "tfms = get_transforms( max_lighting = 0.25) # or tfms=None if none are needed\n",
    "size=(800,1040) # size=(224,224) or (400,224)\n",
    "data = src.transform(tfms=tfms, size=size, resize_method=ResizeMethod.SQUISH).databunch(num_workers=4).normalize(imagenet_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_grade = cnn_learner(data,models.resnet34, pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_grade.load('gradable_res34_removed_best_heat_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load actual full UKBB data\n",
    "crop_paths = [C_CLSA_DIR / 'baseline_left',\n",
    "              C_CLSA_DIR / 'baseline_right',\n",
    "              C_CLSA_DIR / 'followup_left',\n",
    "              C_CLSA_DIR / 'followup_right'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_names = [os.path.basename(pth) for pth in crop_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sets = [ImageList.from_folder(path) for path in crop_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.functional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_preds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i,test in enumerate(test_sets):\n",
    "    learn_grade.data.add_test(test)\n",
    "    pred = learn_grade.get_preds(DatasetType.Test)\n",
    "    gr_preds[os.path.basename(crop_paths[i])] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = WORKING_DIR /\"CLSA_gradabillity_predictions_v2.pkl\"\n",
    "pkl.dump(gr_preds,open(save_path,'wb'))\n",
    "#preds = pkl.load(open(save_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg_preds = pkl.load(open(save_path,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VCDR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_img = TRAIN_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = ImageList.from_folder(str(path_img)).split_by_rand_pct(seed=42).label_from_func(lambda o: (o.parts if isinstance(o, Path) else o.split(os.path.sep))[-2],label_cls=FloatList)\n",
    "tfms = get_transforms(max_rotate= 10,flip_vert =True, max_lighting = 0.1) # or tfms=None if none are needed\n",
    "size=(800,1040) # size=(224,224) or (400,224)\n",
    "data = src.transform(tfms=tfms, size=size, resize_method=ResizeMethod.SQUISH).databunch(num_workers=4).normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_vcdr = cnn_learner(data,models.resnet34, pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_vcdr.load('g_model_e9_stage_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='259' class='' max='385' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      67.27% [259/385 28:56<14:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vc_preds = {}\n",
    "for i,test in enumerate(test_sets):\n",
    "    learn_vcdr.data.add_test(test)\n",
    "    pred = learn_vcdr.get_preds(DatasetType.Test)\n",
    "    vc_preds[os.path.basename(crop_paths[i])] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = WORKING_DIR /\"CLSA_vcdr_predictions_2.pkl\"\n",
    "pkl.dump(vc_preds,open(save_path,'wb'))\n",
    "#preds = pkl.load(open(save_path, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_test_set = {}\n",
    "for i,ts in enumerate(test_sets):\n",
    "    ts_list = list(ts.items)\n",
    "    ts_list = [os.path.basename(x) for x in ts_list]\n",
    "    combined_test_set[crop_names[i]] = ts_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df_result(file_names, preds, preds2):\n",
    "    \n",
    "    file_names = np.asarray(file_names)\n",
    "    file_names = file_names.reshape(len(file_names),1)\n",
    "    pred_arr = preds.numpy()\n",
    "    preds2_arr = preds2.numpy()\n",
    "    print(file_names.shape)\n",
    "    print(preds.shape)\n",
    "    print(preds2.shape)\n",
    "    full_arr = np.concatenate((file_names,pred_arr,preds2_arr),axis=1)\n",
    "    df = pd.DataFrame(full_arr)\n",
    "    df.set_index(0,inplace = True)\n",
    "    df = df.rename(columns = {0:'file',1:'gradable',2:'ungradable',3:'vcdr_estimate'})\n",
    "    df.index.rename(\"file_name\",inplace =True)\n",
    "    return df\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_left\n",
      "(28638, 1)\n",
      "torch.Size([28638, 2])\n",
      "torch.Size([28638, 1])\n",
      "baseline_right\n",
      "(28739, 1)\n",
      "torch.Size([28739, 2])\n",
      "torch.Size([28739, 1])\n",
      "followup_left\n",
      "(24362, 1)\n",
      "torch.Size([24362, 2])\n",
      "torch.Size([24362, 1])\n",
      "followup_right\n",
      "(24591, 1)\n",
      "torch.Size([24591, 2])\n",
      "torch.Size([24591, 1])\n"
     ]
    }
   ],
   "source": [
    "keys = list(combined_test_set.keys())\n",
    "df_dict = {}\n",
    "for key in keys:\n",
    "    print(key)\n",
    "    df = build_df_result(combined_test_set[key], gr_preds[key][0],vc_preds[key][0])\n",
    "    df_dict[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat([i for i in df_dict.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_baseline = pd.concat([df_dict['baseline_left'],df_dict['baseline_right']])\n",
    "df_full_followup = pd.concat([df_dict['followup_left'],df_dict['followup_right']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_baseline['follow_up'] = 0\n",
    "df_full_followup['follow_up'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat([df_full_baseline.reset_index(), df_full_followup.reset_index()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.to_csv(WORKING_DIR /\"CLSA_full_inference_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>gradable</th>\n",
       "      <th>ungradable</th>\n",
       "      <th>vcdr_estimate</th>\n",
       "      <th>follow_up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1845</td>\n",
       "      <td>190225_QIMR_SMacGregor_6344668_left.jpg</td>\n",
       "      <td>0.99500895</td>\n",
       "      <td>0.0049910997</td>\n",
       "      <td>10.615381</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5287</td>\n",
       "      <td>190225_QIMR_SMacGregor_5185392_left.jpg</td>\n",
       "      <td>0.9477532</td>\n",
       "      <td>0.052246768</td>\n",
       "      <td>10.053989</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8423</td>\n",
       "      <td>190225_QIMR_SMacGregor_8151706_left.jpg</td>\n",
       "      <td>0.9423977</td>\n",
       "      <td>0.057602316</td>\n",
       "      <td>10.179149</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14227</td>\n",
       "      <td>190225_QIMR_SMacGregor_2009514_left.jpg</td>\n",
       "      <td>0.9906679</td>\n",
       "      <td>0.009332126</td>\n",
       "      <td>10.302616</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16758</td>\n",
       "      <td>190225_QIMR_SMacGregor_5836860_left.jpg</td>\n",
       "      <td>0.9930233</td>\n",
       "      <td>0.0069767567</td>\n",
       "      <td>10.442637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23870</td>\n",
       "      <td>190225_QIMR_SMacGregor_5364897_left.jpg</td>\n",
       "      <td>0.96118855</td>\n",
       "      <td>0.03881139</td>\n",
       "      <td>10.268616</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27890</td>\n",
       "      <td>190225_QIMR_SMacGregor_3508853_left.jpg</td>\n",
       "      <td>0.9940221</td>\n",
       "      <td>0.005977941</td>\n",
       "      <td>10.038216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8808</td>\n",
       "      <td>190225_QIMR_SMacGregor_9471889_left.jpg</td>\n",
       "      <td>0.5280808</td>\n",
       "      <td>0.47191918</td>\n",
       "      <td>10.567476</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14290</td>\n",
       "      <td>190225_QIMR_SMacGregor_5836860_left.jpg</td>\n",
       "      <td>0.9944944</td>\n",
       "      <td>0.0055056387</td>\n",
       "      <td>10.79579</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22895</td>\n",
       "      <td>190225_QIMR_SMacGregor_6032271_left.jpg</td>\n",
       "      <td>0.9853279</td>\n",
       "      <td>0.014672048</td>\n",
       "      <td>10.297196</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     file_name    gradable    ungradable  \\\n",
       "1845   190225_QIMR_SMacGregor_6344668_left.jpg  0.99500895  0.0049910997   \n",
       "5287   190225_QIMR_SMacGregor_5185392_left.jpg   0.9477532   0.052246768   \n",
       "8423   190225_QIMR_SMacGregor_8151706_left.jpg   0.9423977   0.057602316   \n",
       "14227  190225_QIMR_SMacGregor_2009514_left.jpg   0.9906679   0.009332126   \n",
       "16758  190225_QIMR_SMacGregor_5836860_left.jpg   0.9930233  0.0069767567   \n",
       "23870  190225_QIMR_SMacGregor_5364897_left.jpg  0.96118855    0.03881139   \n",
       "27890  190225_QIMR_SMacGregor_3508853_left.jpg   0.9940221   0.005977941   \n",
       "8808   190225_QIMR_SMacGregor_9471889_left.jpg   0.5280808    0.47191918   \n",
       "14290  190225_QIMR_SMacGregor_5836860_left.jpg   0.9944944  0.0055056387   \n",
       "22895  190225_QIMR_SMacGregor_6032271_left.jpg   0.9853279   0.014672048   \n",
       "\n",
       "      vcdr_estimate  follow_up  \n",
       "1845      10.615381          0  \n",
       "5287      10.053989          0  \n",
       "8423      10.179149          0  \n",
       "14227     10.302616          0  \n",
       "16758     10.442637          0  \n",
       "23870     10.268616          0  \n",
       "27890     10.038216          0  \n",
       "8808      10.567476          1  \n",
       "14290      10.79579          1  \n",
       "22895     10.297196          1  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full[pd.to_numeric(df_full['vcdr_estimate']) > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(df_dict,open(WORKING_DIR / \"UKBB_inference_python_dictionaries.pkl\",'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glaucoma",
   "language": "python",
   "name": "glaucoma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}